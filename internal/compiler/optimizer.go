package compiler

import (
	"github.com/ATSOTECK/rage/internal/runtime"
)

// Optimizer performs various compile-time optimizations
type Optimizer struct {
	enabled bool
}

// NewOptimizer creates a new optimizer
func NewOptimizer() *Optimizer {
	return &Optimizer{enabled: true}
}

// ==========================================
// Peephole Optimizer - Post-process bytecode
// ==========================================

// PeepholeOptimize optimizes the generated bytecode
func (o *Optimizer) PeepholeOptimize(code *runtime.CodeObject) {
	if !o.enabled || len(code.Code) == 0 {
		return
	}

	// Parse bytecode into instructions
	instrs := o.parseInstructions(code)

	// Apply peephole patterns
	changed := true
	for changed {
		changed = false

		// Remove redundant LOAD/POP pairs
		changed = o.removeLoadPop(instrs) || changed

		// Remove DUP/POP pairs
		changed = o.removeDupPop(instrs) || changed

		// Optimize jumps
		changed = o.optimizeJumps(instrs, code) || changed

		// Convert LOAD_FAST to specialized versions
		changed = o.specializeLoadFast(instrs) || changed

		// Convert STORE_FAST to specialized versions
		changed = o.specializeStoreFast(instrs) || changed

		// Convert constant loads to specialized versions
		changed = o.specializeLoadConst(instrs, code) || changed

		// Detect and convert increment patterns
		changed = o.detectIncrementPattern(instrs, code) || changed

		// Detect and convert decrement patterns
		changed = o.detectDecrementPattern(instrs, code) || changed

		// Detect and convert negate-in-place patterns (sign = -sign)
		changed = o.detectNegatePattern(instrs, code) || changed

		// Detect and convert add-const patterns (x = x + const)
		changed = o.detectAddConstPattern(instrs, code) || changed

		// Detect LOAD_FAST LOAD_FAST superinstruction
		changed = o.detectLoadFastLoadFast(instrs) || changed

		// Detect LOAD_FAST LOAD_CONST superinstruction
		changed = o.detectLoadFastLoadConst(instrs, code) || changed

		// Detect LOAD_CONST LOAD_FAST superinstruction
		changed = o.detectLoadConstLoadFast(instrs, code) || changed

		// Detect STORE_FAST LOAD_FAST superinstruction
		changed = o.detectStoreFastLoadFast(instrs) || changed

		// Optimize empty collection building
		changed = o.optimizeEmptyCollections(instrs) || changed

		// Optimize compare+jump sequences
		changed = o.optimizeCompareJump(instrs) || changed

		// Detect compare-local-jump fusion (for while loop conditions)
		changed = o.detectCompareLtLocalJump(instrs, code) || changed

		// Store-load elimination (STORE x; LOAD x -> DUP; STORE x)
		changed = o.eliminateStoreLoad(instrs) || changed

		// Jump threading
		changed = o.threadJumps(instrs) || changed

		// Optimize len() calls on known types
		changed = o.optimizeLenCalls(instrs, code) || changed

		// Specialize binary operations for integers
		changed = o.SpecializeBinaryOps(instrs, code) || changed
	}

	// Rebuild bytecode from instructions
	o.rebuildBytecode(code, instrs)
}

type instruction struct {
	op             runtime.Opcode
	arg            int
	removed        bool
	originalHadArg bool // Track if original instruction had an argument (for offset calculation)
}

func (o *Optimizer) parseInstructions(code *runtime.CodeObject) []*instruction {
	var instrs []*instruction
	offset := 0
	for offset < len(code.Code) {
		op := runtime.Opcode(code.Code[offset])
		hadArg := op.HasArg() && offset+2 < len(code.Code)
		instr := &instruction{op: op, arg: -1, originalHadArg: hadArg}
		if hadArg {
			instr.arg = int(code.Code[offset+1]) | int(code.Code[offset+2])<<8
			offset += 3
		} else {
			offset++
		}
		instrs = append(instrs, instr)
	}
	return instrs
}

func (o *Optimizer) rebuildBytecode(code *runtime.CodeObject, instrs []*instruction) {
	// Calculate new size
	size := 0
	for _, instr := range instrs {
		if instr.removed {
			continue
		}
		if instr.arg >= 0 {
			size += 3
		} else {
			size++
		}
	}

	// Build offset map for jump target adjustment
	oldOffsets := make([]int, len(instrs))
	newOffsets := make([]int, len(instrs))
	oldOffset := 0
	newOffset := 0
	for i, instr := range instrs {
		oldOffsets[i] = oldOffset
		newOffsets[i] = newOffset
		// Use originalHadArg to correctly calculate old bytecode offsets
		if instr.originalHadArg {
			oldOffset += 3
		} else {
			oldOffset++
		}
		if !instr.removed {
			if instr.arg >= 0 {
				newOffset += 3
			} else {
				newOffset++
			}
		}
	}

	// Rebuild bytecode
	newCode := make([]byte, 0, size)
	for _, instr := range instrs {
		if instr.removed {
			continue
		}

		// Adjust jump targets
		arg := instr.arg
		if isJumpOp(instr.op) && arg >= 0 {
			// Find which instruction index the old offset points to
			targetIdx := -1
			for j, oldOff := range oldOffsets {
				if oldOff == arg {
					targetIdx = j
					break
				}
			}

			if targetIdx >= 0 {
				// If target instruction was removed, find next valid instruction
				for targetIdx < len(instrs) && instrs[targetIdx].removed {
					targetIdx++
				}
				if targetIdx < len(instrs) {
					arg = newOffsets[targetIdx]
				}
			}
		}

		newCode = append(newCode, byte(instr.op))
		if arg >= 0 {
			newCode = append(newCode, byte(arg), byte(arg>>8))
		}
	}

	code.Code = newCode
}

func isJumpOp(op runtime.Opcode) bool {
	switch op {
	case runtime.OpJump, runtime.OpJumpIfTrue, runtime.OpJumpIfFalse,
		runtime.OpPopJumpIfTrue, runtime.OpPopJumpIfFalse,
		runtime.OpJumpIfTrueOrPop, runtime.OpJumpIfFalseOrPop, runtime.OpContinueLoop,
		runtime.OpForIter, runtime.OpSetupExcept, runtime.OpSetupFinally,
		runtime.OpSetupExceptStar, runtime.OpSetupWith,
		// New compare+jump superinstructions
		runtime.OpCompareLtJump, runtime.OpCompareLeJump,
		runtime.OpCompareGtJump, runtime.OpCompareGeJump,
		runtime.OpCompareEqJump, runtime.OpCompareNeJump,
		runtime.OpCompareLtLocalJump:
		return true
	}
	return false
}
